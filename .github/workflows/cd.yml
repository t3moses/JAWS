# Continuous Deployment workflow
# REQUIRED SETUP:
# 1. Create "production" environment in: Repo Settings ‚Üí Environments ‚Üí New environment
# 2. Add required reviewers (e.g., "robjohnston", "t3moses")
# 3. Optionally set deployment branches to "main" only
# 4. Add secrets: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY

name: CD Pipeline

on:
  workflow_run:
    workflows: ["CI Pipeline"]
    types: [completed]
    branches: [main]

permissions:
  contents: read
  actions: read

concurrency:
  group: production-${{ github.event.workflow_run.head_branch }}
  cancel-in-progress: false

jobs:
  check-ci-status:
    name: Verify CI Pipeline Passed
    runs-on: ubuntu-latest
    # Only proceed if CI succeeded
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
    outputs:
      ci_passed: ${{ steps.check.outputs.ci_passed }}
      branch: ${{ steps.check.outputs.branch }}
      run_id: ${{ steps.check.outputs.run_id }}
    steps:
      - name: Check CI status and capture metadata
        id: check
        run: |
          echo "‚úÖ CI pipeline passed - ready for deployment approval"
          echo "ci_passed=true" >> $GITHUB_OUTPUT
          echo "branch=${{ github.event.workflow_run.head_branch }}" >> $GITHUB_OUTPUT
          echo "run_id=${{ github.event.workflow_run.id }}" >> $GITHUB_OUTPUT

          echo "üìã CI Run Details:"
          echo "  - Branch: ${{ github.event.workflow_run.head_branch }}"
          echo "  - Run ID: ${{ github.event.workflow_run.id }}"
          echo "  - Commit: ${{ github.event.workflow_run.head_sha }}"
          echo "  - Event: ${{ github.event.workflow_run.event }}"

  ci-failed-notification:
    name: CI Failed - Deployment Cancelled
    runs-on: ubuntu-latest
    # Only run if CI failed
    if: ${{ github.event.workflow_run.conclusion != 'success' }}
    steps:
      - name: Report CI failure
        run: |
          echo "‚ùå ================================"
          echo "‚ùå CI PIPELINE FAILED"
          echo "‚ùå ================================"
          echo "Deployment to production has been cancelled."
          echo ""
          echo "üìã CI Run Details:"
          echo "  - Branch: ${{ github.event.workflow_run.head_branch }}"
          echo "  - Run ID: ${{ github.event.workflow_run.id }}"
          echo "  - Conclusion: ${{ github.event.workflow_run.conclusion }}"
          echo "  - Event: ${{ github.event.workflow_run.event }}"
          echo ""
          echo "üîç Next Steps:"
          echo "  1. Review the failed CI workflow run"
          echo "  2. Fix any test failures or linting issues"
          echo "  3. Push your fixes to trigger a new CI run"
          echo "  4. CD will automatically trigger when CI passes"
          exit 1

  deploy-production:
    name: Deploy to AWS Production
    runs-on: ubuntu-latest
    needs: check-ci-status

    # This environment configuration requires approval
    # IMPORTANT: If deployment doesn't wait for approval, the "production" environment
    # may not be configured. See setup instructions at the top of this file.
    environment:
      name: production
      url: https://nsc-sdc.ca   # a link to view the deployed environment (no impact on deployment).

    steps:
      - name: Deployment environment check
        run: |
          echo "üöÄ Starting deployment to production"
          echo "üìã Deployment Details:"
          echo "  - Branch: ${{ needs.check-ci-status.outputs.branch }}"
          echo "  - CI Run ID: ${{ needs.check-ci-status.outputs.run_id }}"
          echo "  - Environment: production"
          echo ""
          echo "‚ö†Ô∏è  If this deployment started without approval, check that:"
          echo "  1. The 'production' environment exists in repository settings"
          echo "  2. Required reviewers are configured for the environment"
          echo "  3. Deployment branch rules are set (if applicable)"

      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.workflow_run.head_sha }}

      - name: Download vendor artifacts
        uses: actions/download-artifact@v4
        with:
          name: vendor-artifacts
          path: vendor
          run-id: ${{ github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: false

      - name: Verify vendor artifacts
        run: |
          if [ ! -d "vendor" ] || [ -z "$(ls -A vendor)" ]; then
            echo "‚ùå ERROR: Vendor artifacts not found or empty"
            echo "This may indicate:"
            echo "  1. Artifacts expired (retention: 1 day)"
            echo "  2. CI workflow didn't upload artifacts successfully"
            echo "  3. Artifact download failed"
            exit 1
          fi
          echo "‚úÖ Vendor artifacts verified"

      - name: Download database artifacts
        uses: actions/download-artifact@v4
        with:
          name: database-artifacts
          path: database/
          run-id: ${{ github.event.workflow_run.id }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
        continue-on-error: false

      - name: Verify database artifacts
        run: |
          if [ ! -f "database/jaws.db" ]; then
            echo "‚ùå ERROR: Database artifact not found"
            echo "This may indicate:"
            echo "  1. Artifacts expired (retention: 1 day)"
            echo "  2. CI workflow didn't upload database successfully"
            echo "  3. Artifact download failed"
            exit 1
          fi
          echo "‚úÖ Database artifacts verified"
          ls -lh database/jaws.db

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ca-central-1

      - name: Verify AWS credentials
        run: |
          if ! aws sts get-caller-identity > /dev/null 2>&1; then
            echo "‚ùå ERROR: AWS credentials invalid or not configured"
            echo "Check that AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are set correctly"
            exit 1
          fi
          echo "‚úÖ AWS credentials verified"
          aws sts get-caller-identity

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform init (local state)
        run: |
          echo "üîß Initializing Terraform with local state..."
          terraform init
        working-directory: terraform

      - name: Import existing backend resources
        run: |
          echo "üîÑ Checking for existing backend resources..."

          # Get AWS account ID for bucket name
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          BUCKET_NAME="jaws-terraform-state-${ACCOUNT_ID}"
          echo "TERRAFORM_BUCKET=$BUCKET_NAME" >> $GITHUB_ENV
          echo "Expected bucket name: $BUCKET_NAME"

          # Try to import S3 bucket
          terraform import -input=false aws_s3_bucket.terraform_state "$BUCKET_NAME" 2>&1 || echo "‚ÑπÔ∏è  Bucket import skipped (may not exist or already imported)"

          # Try to import bucket versioning
          terraform import -input=false aws_s3_bucket_versioning.terraform_state "$BUCKET_NAME" 2>&1 || echo "‚ÑπÔ∏è  Versioning import skipped"

          # Try to import bucket encryption
          terraform import -input=false aws_s3_bucket_server_side_encryption_configuration.terraform_state "$BUCKET_NAME" 2>&1 || echo "‚ÑπÔ∏è  Encryption import skipped"

          # Try to import public access block
          terraform import -input=false aws_s3_bucket_public_access_block.terraform_state "$BUCKET_NAME" 2>&1 || echo "‚ÑπÔ∏è  Public access block import skipped"

          # Try to import DynamoDB table
          terraform import -input=false aws_dynamodb_table.terraform_locks "jaws-terraform-locks" 2>&1 || echo "‚ÑπÔ∏è  DynamoDB table import skipped"

          echo "‚úÖ Import check complete"
        working-directory: terraform
        continue-on-error: true

      - name: Create S3 backend resources
        run: |
          echo "üì¶ Creating S3 bucket and DynamoDB table for state storage (if needed)..."

          # Plan and apply to create backend resources
          terraform plan -target=aws_s3_bucket.terraform_state \
                         -target=aws_s3_bucket_versioning.terraform_state \
                         -target=aws_s3_bucket_server_side_encryption_configuration.terraform_state \
                         -target=aws_s3_bucket_public_access_block.terraform_state \
                         -target=aws_dynamodb_table.terraform_locks \
                         -out=backend.tfplan

          terraform apply -auto-approve backend.tfplan

          echo "‚úÖ Backend resources ready: $TERRAFORM_BUCKET"
        working-directory: terraform

      - name: Configure S3 backend
        run: |
          echo "üîß Configuring S3 backend..."

          # Uncomment and update backend configuration
          sed -i 's/# terraform {/terraform {/' backend_config.tf
          sed -i 's/#   backend "s3" {/  backend "s3" {/' backend_config.tf
          sed -i "s/#     bucket         = .*/    bucket         = \"$TERRAFORM_BUCKET\"/" backend_config.tf
          sed -i 's/#     key            = /    key            = /' backend_config.tf
          sed -i 's/#     region         = /    region         = /' backend_config.tf
          sed -i 's/#     encrypt        = /    encrypt        = /' backend_config.tf
          sed -i 's/#     dynamodb_table = /    dynamodb_table = /' backend_config.tf
          sed -i 's/#   }/  }/' backend_config.tf
          sed -i 's/# }/}/' backend_config.tf

          echo "‚úÖ Backend configuration updated"
          cat backend_config.tf
        working-directory: terraform

      - name: Migrate state to S3
        run: |
          echo "üì¶ Migrating Terraform state to S3..."
          terraform init -migrate-state -force-copy
          echo "‚úÖ State migrated to S3 backend"
        working-directory: terraform

      - name: Import existing Lightsail instance
        run: |
          echo "üîÑ Checking for existing Lightsail instance..."

          # Import Lightsail instance
          # Note: Static IP, IP attachment, and public ports don't support import
          # and are managed manually in AWS
          echo "Importing instance..."
          if terraform import -input=false aws_lightsail_instance.app jaws-production 2>&1 | tee /tmp/import_instance.log; then
            echo "‚úÖ Instance imported"
          else
            if grep -q "Resource already managed" /tmp/import_instance.log; then
              echo "‚ÑπÔ∏è  Instance already in state"
            elif grep -q "Cannot import non-existent" /tmp/import_instance.log; then
              echo "‚ÑπÔ∏è  Instance doesn't exist yet - will be created"
            else
              echo "‚ö†Ô∏è  Instance import failed"
              cat /tmp/import_instance.log
            fi
          fi

          echo "‚úÖ Lightsail import check complete"
        working-directory: terraform
        continue-on-error: true

      - name: Terraform plan
        run: |
          echo "üìã Planning Terraform changes..."
          terraform plan -out tfplan
        working-directory: terraform

      - name: Terraform apply
        run: |
          echo "üöÄ Applying Terraform configuration..."
          terraform apply -auto-approve tfplan
          echo "‚úÖ Infrastructure provisioned successfully"
        working-directory: terraform

      # If you replace the auto-generated key pair with an existing key pair,
      # update this step to write the private key from a secret instead.
      - name: Prepare SSH key
        run: |
          echo "üîë Retrieving SSH key from Terraform..."
          SSH_PRIVATE_KEY="$(terraform output -raw ssh_private_key)"
          if [ -z "$SSH_PRIVATE_KEY" ]; then
            echo "‚ùå ERROR: Failed to retrieve SSH private key from Terraform"
            exit 1
          fi
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/lightsail.pem
          chmod 600 ~/.ssh/lightsail.pem
          echo "‚úÖ SSH key configured"
        working-directory: terraform

      - name: Get instance IP
        id: terraform-outputs
        run: |
          echo "üåê Retrieving instance IP from Terraform..."
          INSTANCE_IP="$(terraform output -raw instance_ip)"
          if [ -z "$INSTANCE_IP" ]; then
            echo "‚ùå ERROR: Failed to retrieve instance IP from Terraform"
            exit 1
          fi
          echo "instance_ip=$INSTANCE_IP" >> $GITHUB_OUTPUT
          echo "‚úÖ Instance IP: $INSTANCE_IP"
        working-directory: terraform

      - name: Add host to known_hosts
        run: |
          echo "üîê Adding host to known_hosts..."
          ssh-keyscan -H ${{ steps.terraform-outputs.outputs.instance_ip }} >> ~/.ssh/known_hosts
          echo "‚úÖ Host key verified"

      - name: Wait for instance initialization
        run: |
          echo "‚è≥ Waiting for instance to complete initialization..."
          INSTANCE_IP="${{ steps.terraform-outputs.outputs.instance_ip }}"
          MAX_ATTEMPTS=30
          ATTEMPT=0

          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            echo "Attempt $((ATTEMPT + 1))/$MAX_ATTEMPTS: Checking if instance is ready..."

            if ssh -i ~/.ssh/lightsail.pem -o ConnectTimeout=10 -o StrictHostKeyChecking=no \
                ubuntu@$INSTANCE_IP "test -d /var/www/html && systemctl is-active apache2" 2>/dev/null; then
              echo "‚úÖ Instance is ready!"
              break
            fi

            if [ $ATTEMPT -eq $((MAX_ATTEMPTS - 1)) ]; then
              echo "‚ùå ERROR: Instance failed to initialize within expected time"
              echo "Checking instance status..."
              ssh -i ~/.ssh/lightsail.pem -o ConnectTimeout=10 ubuntu@$INSTANCE_IP "ls -la /var/ || echo 'Directory check failed'" || echo "SSH connection failed"
              exit 1
            fi

            echo "Not ready yet, waiting 10 seconds..."
            sleep 10
            ATTEMPT=$((ATTEMPT + 1))
          done

      - name: Deploy backend
        run: |
          echo "üöÄ Deploying backend to production..."
          ssh -i ~/.ssh/lightsail.pem ubuntu@${{ steps.terraform-outputs.outputs.instance_ip }} << 'EOF'
            set -e

            echo "üìÇ Ensuring web directory exists..."
            if [ ! -d "/var/www/html" ]; then
              echo "‚ö†Ô∏è  Web directory not found, creating it..."
              sudo mkdir -p /var/www/html
              sudo chown -R ubuntu:www-data /var/www/html
            fi

            cd /var/www/html

            echo "üì• Checking repository status..."
            if [ ! -d ".git" ]; then
              echo "üì¶ Initializing repository..."
              # Remove any existing files (like default Apache index.html)
              sudo rm -rf ./*
              git clone https://github.com/RobJohnston/JAWS.git .
            else
              echo "üì• Pulling latest code..."
              git fetch origin
              git reset --hard origin/${{ needs.check-ci-status.outputs.branch }}
            fi

            echo "üì¶ Installing dependencies..."
            composer install --no-dev --optimize-autoloader

            echo "üóÑÔ∏è  Setting up database directory..."
            mkdir -p database
            sudo chown -R www-data:www-data database
            sudo chmod -R 775 database

            echo "üóÑÔ∏è  Running database migrations..."
            vendor/bin/phinx migrate -e production

            echo "üîÑ Restarting web server..."
            sudo systemctl restart apache2 || sudo /opt/bitnami/ctlscript.sh restart apache

            echo "‚úÖ Backend deployment complete"
          EOF

      - name: Deploy frontend
        run: |
          echo "üé® Deploying frontend to production..."
          rsync -avz -e "ssh -i ~/.ssh/lightsail.pem" \
            --exclude '.git' \
            --exclude 'node_modules' \
            --exclude 'tests' \
            --exclude '.env' \
            public/ ubuntu@${{ steps.terraform-outputs.outputs.instance_ip }}:/var/www/html/

          echo "üîí Setting file permissions..."
          ssh -i ~/.ssh/lightsail.pem ubuntu@${{ steps.terraform-outputs.outputs.instance_ip }} << 'EOF'
            sudo chown -R www-data:www-data /var/www/html
            sudo chmod -R 755 /var/www/html
          EOF
          echo "‚úÖ Frontend deployment complete"

      - name: Smoke tests
        run: |
          echo "üß™ Running smoke tests..."
          sleep 5

          echo "Testing homepage..."
          if ! curl -f http://${{ steps.terraform-outputs.outputs.instance_ip }}/; then
            echo "‚ùå Homepage test failed"
            exit 1
          fi
          echo "‚úÖ Homepage is accessible"

          echo "Testing API endpoint..."
          if ! curl -f http://${{ steps.terraform-outputs.outputs.instance_ip }}/api/events; then
            echo "‚ùå API test failed"
            exit 1
          fi
          echo "‚úÖ API is responding"

          echo "üéâ All smoke tests passed!"

      - name: Deployment summary
        if: always()
        run: |
          if [ "${{ job.status }}" == "success" ]; then
            echo "‚úÖ ================================"
            echo "‚úÖ DEPLOYMENT SUCCESSFUL"
            echo "‚úÖ ================================"
            echo "üìã Deployment Details:"
            echo "  - Branch: ${{ needs.check-ci-status.outputs.branch }}"
            echo "  - Environment: production"
            echo "  - URL: https://nsc-sdc.ca"
            echo "  - Instance IP: ${{ steps.terraform-outputs.outputs.instance_ip }}"
          else
            echo "‚ùå ================================"
            echo "‚ùå DEPLOYMENT FAILED"
            echo "‚ùå ================================"
            echo "üìã Troubleshooting:"
            echo "  - Review the failed step above"
            echo "  - Check AWS credentials and permissions"
            echo "  - Verify Terraform state is valid"
            echo "  - Check instance connectivity"
            echo "  - Review application logs on the server"
          fi
